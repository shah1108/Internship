{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d97a9b",
   "metadata": {},
   "source": [
    "# ASSIGNMENT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f298b57",
   "metadata": {},
   "source": [
    "## WEB SCRAPING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6ff89c",
   "metadata": {},
   "source": [
    "### Q1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2313e5b0",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bee90410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.4.3)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "# installing selenuim\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29cf3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fecae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to chromedriver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d39b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the webpage\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58a2c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys(\"Data Analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "960de1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input')\n",
    "location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1b6a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the search button.\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[6]\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c200222",
   "metadata": {},
   "source": [
    "* lets move to scraping part as i reached to the desied page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a5b80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first creating empty list to store the data\n",
    "job_title = []\n",
    "job_location = []\n",
    "company_name = []\n",
    "experience_required = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52c009ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the data for the first 10 jobs results you get.\n",
    "# scraping job title from given page\n",
    "title_tags = driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title = i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "# scraping job location from given page\n",
    "location_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location = i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "# scraping company name from the given page\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company = i.text\n",
    "    company_name.append(company)\n",
    "\n",
    "# scraping job experience for the given page\n",
    "experience_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in experience_tags[0:10]:\n",
    "    experience = i.text\n",
    "    experience_required.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39978d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "# checking length of our lists\n",
    "print(len(job_title),len(job_location),len(company_name),len(experience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dd1367f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for “Data Analyst” Job position in “Bangalore” location :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB TITLE</th>\n",
       "      <th>LOB LOCATION</th>\n",
       "      <th>COMPANY NAME</th>\n",
       "      <th>EXPERIENCE REQUIRED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Augie Pets India Pvt limited</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Novel Office</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BYJU'S DBEL : Sr Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>BYJUS</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SR. Data Analyst- SME- Pharma and Healthcare</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Hyderabad/Secunder...</td>\n",
       "      <td>CHRYSELYS</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Business Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Varite</td>\n",
       "      <td>8-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Analyst - KPO</td>\n",
       "      <td>Bangalore/Bengaluru, Gurgaon/Gurugram, Gurgaon...</td>\n",
       "      <td>Huquo Consulting Pvt. Ltd</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst 2</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Optum</td>\n",
       "      <td>7-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst 1</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Optum</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Associate Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Optum</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>ANZ</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       JOB TITLE  \\\n",
       "1                                   Data Analyst   \n",
       "2                                   Data Analyst   \n",
       "3                  BYJU'S DBEL : Sr Data Analyst   \n",
       "4   SR. Data Analyst- SME- Pharma and Healthcare   \n",
       "5                          Business Data Analyst   \n",
       "6                      Senior Data Analyst - KPO   \n",
       "7                                 Data Analyst 2   \n",
       "8                                 Data Analyst 1   \n",
       "9                         Associate Data Analyst   \n",
       "10                                  Data Analyst   \n",
       "\n",
       "                                         LOB LOCATION  \\\n",
       "1                                 Bangalore/Bengaluru   \n",
       "2                                 Bangalore/Bengaluru   \n",
       "3                                 Bangalore/Bengaluru   \n",
       "4   Bangalore/Bengaluru, Noida, Hyderabad/Secunder...   \n",
       "5                                 Bangalore/Bengaluru   \n",
       "6   Bangalore/Bengaluru, Gurgaon/Gurugram, Gurgaon...   \n",
       "7                                 Bangalore/Bengaluru   \n",
       "8                                 Bangalore/Bengaluru   \n",
       "9                                 Bangalore/Bengaluru   \n",
       "10                                Bangalore/Bengaluru   \n",
       "\n",
       "                    COMPANY NAME EXPERIENCE REQUIRED  \n",
       "1   Augie Pets India Pvt limited             0-3 Yrs  \n",
       "2                   Novel Office             0-3 Yrs  \n",
       "3                          BYJUS             3-7 Yrs  \n",
       "4                      CHRYSELYS             2-6 Yrs  \n",
       "5                         Varite            8-11 Yrs  \n",
       "6      Huquo Consulting Pvt. Ltd            7-12 Yrs  \n",
       "7                          Optum            7-10 Yrs  \n",
       "8                          Optum             3-5 Yrs  \n",
       "9                          Optum             4-7 Yrs  \n",
       "10                           ANZ             1-5 Yrs  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for our data\n",
    "df = pd.DataFrame({'JOB TITLE':job_title,'LOB LOCATION':job_location,'COMPANY NAME':company_name,'EXPERIENCE REQUIRED':experience_required},index=list(range(1,11)))\n",
    "print(\"Data for “Data Analyst” Job position in “Bangalore” location :\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00fa131",
   "metadata": {},
   "source": [
    "### Q2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b3cc2d",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a46a9933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.4.3)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "# installing selenium\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f35044ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa931dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first connect to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d1ca194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the naukri page on automated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9137b5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter desigation and location as required in the question\n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8c094b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87d0d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the search button.\n",
    "search = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c805a",
   "metadata": {},
   "source": [
    "* lets move to scraping part as i reached to the desied page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4233597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first creating empty list to store the data\n",
    "job_title2 = []\n",
    "job_location2 = []\n",
    "company_name2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bbecf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping job title from given page\n",
    "title_tags2 = driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags2[0:10]:\n",
    "    title2 = i.text\n",
    "    job_title2.append(title2)\n",
    "    \n",
    "# scraping job location from given page\n",
    "location_tags2 = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags2[0:10]:\n",
    "    location2 = i.text\n",
    "    job_location2.append(location2)\n",
    "    \n",
    "# scraping company name from the given page\n",
    "company_tags2 = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags2[0:10]:\n",
    "    company2 = i.text\n",
    "    company_name2.append(company2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86b7ba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "# checking length of our lists\n",
    "print(len(job_title2),len(job_location2),len(company_name2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "290542f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for “Data Scientist” Job position in “Bangalore” location :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB TITLE</th>\n",
       "      <th>LOB LOCATION</th>\n",
       "      <th>COMPANY NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hiring For DATA Scientist @ NTT DATA Business ...</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Hyderabad/Secunder...</td>\n",
       "      <td>NTT DATA Business Solutions Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Assistant Manager - Data Science</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune</td>\n",
       "      <td>CitiusTech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist / Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Pune, Chennai</td>\n",
       "      <td>Mindtree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lead ML Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tcs Hiring For Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Chennai, Mumbai (All Areas)</td>\n",
       "      <td>TATA CONSULTANCY SERVICES (TCS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist - II</td>\n",
       "      <td>Bangalore/Bengaluru, India, Mumbai (All Areas)</td>\n",
       "      <td>SMARTPADDLE TECHNOLOGY PVT. LTD.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ACN - Applied Intelligence - Data Scientist - 09</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ACN - Applied Intelligence - Data Scientist - 09</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Team Lead/Consultant-Data Science</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            JOB TITLE  \\\n",
       "1                    Analystics & Modeling Specialist   \n",
       "2   Hiring For DATA Scientist @ NTT DATA Business ...   \n",
       "3                    Assistant Manager - Data Science   \n",
       "4              Data Scientist / Senior Data Scientist   \n",
       "5                                   Lead ML Scientist   \n",
       "6                       Tcs Hiring For Data Scientist   \n",
       "7                                 Data Scientist - II   \n",
       "8    ACN - Applied Intelligence - Data Scientist - 09   \n",
       "9    ACN - Applied Intelligence - Data Scientist - 09   \n",
       "10                  Team Lead/Consultant-Data Science   \n",
       "\n",
       "                                         LOB LOCATION  \\\n",
       "1   Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...   \n",
       "2   Bangalore/Bengaluru, Noida, Hyderabad/Secunder...   \n",
       "3                   Bangalore/Bengaluru, Mumbai, Pune   \n",
       "4                  Bangalore/Bengaluru, Pune, Chennai   \n",
       "5                         Bangalore/Bengaluru, Mumbai   \n",
       "6    Bangalore/Bengaluru, Chennai, Mumbai (All Areas)   \n",
       "7      Bangalore/Bengaluru, India, Mumbai (All Areas)   \n",
       "8                                 Bangalore/Bengaluru   \n",
       "9                                 Bangalore/Bengaluru   \n",
       "10                                Bangalore/Bengaluru   \n",
       "\n",
       "                                   COMPANY NAME  \n",
       "1                                     Accenture  \n",
       "2   NTT DATA Business Solutions Private Limited  \n",
       "3                                    CitiusTech  \n",
       "4                                      Mindtree  \n",
       "5                             Fractal Analytics  \n",
       "6               TATA CONSULTANCY SERVICES (TCS)  \n",
       "7              SMARTPADDLE TECHNOLOGY PVT. LTD.  \n",
       "8                                     Accenture  \n",
       "9                                     Accenture  \n",
       "10                                    Accenture  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for our data\n",
    "df = pd.DataFrame({'JOB TITLE':job_title2,'LOB LOCATION':job_location2,'COMPANY NAME':company_name2},index=list(range(1,11)))\n",
    "print(\"Data for “Data Scientist” Job position in “Bangalore” location :\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e804d82",
   "metadata": {},
   "source": [
    "### Q3: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8a1ed9",
   "metadata": {},
   "source": [
    "In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1daeddbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.4.3)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "# installing selenium\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c69bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78fd306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection to webdriver\n",
    "driver = webdriver.Chrome(r'chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8fed4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the naukri page on automated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb20b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter desigation and location as required in the question\n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83347d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the search button.\n",
    "search = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "230b8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply location “Delhi/NCR”\n",
    "location  = driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/section[1]/div[2]/div[5]/div[2]/div[3]/label/p/span[1]\")\n",
    "location.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f22f265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply salary range (3-6Lakhs)\n",
    "salary = driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/section[1]/div[2]/div[6]/div[2]/div[2]/label/p/span[1]\")\n",
    "salary.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e19a6a5",
   "metadata": {},
   "source": [
    "* Now, I have selected all the filters                                             \n",
    "* let's move to scraping part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b311e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first creating empty list to store the data\n",
    "job_title3 = []\n",
    "job_location3 = []\n",
    "company_name3 = []\n",
    "experience_required3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2ffaab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping job title from given page\n",
    "title_tags3 = driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags3[0:10]:\n",
    "    title3 = i.text\n",
    "    job_title3.append(title3)\n",
    "    \n",
    "# scraping job location from given page\n",
    "location_tags3 = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags3[0:10]:\n",
    "    location3 = i.text\n",
    "    job_location3.append(location3)\n",
    "    \n",
    "# scraping company name from the given page\n",
    "company_tags3 = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags3[0:10]:\n",
    "    company3 = i.text\n",
    "    company_name3.append(company3)\n",
    "\n",
    "# scraping job experience for the given page\n",
    "experience_tags3 = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in experience_tags3[0:10]:\n",
    "    experience3 = i.text\n",
    "    experience_required3.append(experience3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "294015b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "# checking length of our lists\n",
    "print(len(job_title3),len(job_location3),len(company_name3),len(experience_required3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3e5df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for “Data Scientist” Job position in “Delhi / NCR” location and Salary (3-6 lakhs):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB TITLE</th>\n",
       "      <th>LOB LOCATION</th>\n",
       "      <th>COMPANY NAME</th>\n",
       "      <th>EXPERIENCE REQUIRED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DigitalBCG GAMMA Data Scientist</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist / Chat-bot Developer</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru, Mumbai (All Ar...</td>\n",
       "      <td>Big Seo Buzz</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Noida(Sector-59 Noida)\\n(WFH during Covid)</td>\n",
       "      <td>R Systems International</td>\n",
       "      <td>7-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Optum</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chat-bot Developer / Data Scientist</td>\n",
       "      <td>Mumbai, New Delhi, Bangalore/Bengaluru\\n(WFH d...</td>\n",
       "      <td>Big Seo Buzz</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - Predictive Analytics</td>\n",
       "      <td>Noida, Mumbai, Chandigarh, Hyderabad/Secundera...</td>\n",
       "      <td>Confidential</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist For Healthcare Product team</td>\n",
       "      <td>Delhi / NCR, Chennai, Bangalore/Bengaluru</td>\n",
       "      <td>SECUREKLOUD TECHNOLOGIES</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>4i Odc</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist - Engine Algorithm</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>Primo Hiring</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Scientist, Associate</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "      <td>NatWest Group</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     JOB TITLE  \\\n",
       "1              DigitalBCG GAMMA Data Scientist   \n",
       "2          Data Scientist / Chat-bot Developer   \n",
       "3                          Lead Data Scientist   \n",
       "4                               Data Scientist   \n",
       "5          Chat-bot Developer / Data Scientist   \n",
       "6        Data Scientist - Predictive Analytics   \n",
       "7   Data Scientist For Healthcare Product team   \n",
       "8                               Data Scientist   \n",
       "9            Data Scientist - Engine Algorithm   \n",
       "10                   Data Scientist, Associate   \n",
       "\n",
       "                                         LOB LOCATION  \\\n",
       "1                      New Delhi, Bangalore/Bengaluru   \n",
       "2   New Delhi, Bangalore/Bengaluru, Mumbai (All Ar...   \n",
       "3          Noida(Sector-59 Noida)\\n(WFH during Covid)   \n",
       "4                                    Gurgaon/Gurugram   \n",
       "5   Mumbai, New Delhi, Bangalore/Bengaluru\\n(WFH d...   \n",
       "6   Noida, Mumbai, Chandigarh, Hyderabad/Secundera...   \n",
       "7           Delhi / NCR, Chennai, Bangalore/Bengaluru   \n",
       "8                                               Noida   \n",
       "9   Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...   \n",
       "10                                        Delhi / NCR   \n",
       "\n",
       "                COMPANY NAME EXPERIENCE REQUIRED  \n",
       "1    Boston Consulting Group             2-5 Yrs  \n",
       "2               Big Seo Buzz             3-7 Yrs  \n",
       "3    R Systems International            7-10 Yrs  \n",
       "4                      Optum             2-7 Yrs  \n",
       "5               Big Seo Buzz             2-7 Yrs  \n",
       "6               Confidential             1-6 Yrs  \n",
       "7   SECUREKLOUD TECHNOLOGIES             2-7 Yrs  \n",
       "8                     4i Odc             2-4 Yrs  \n",
       "9               Primo Hiring             1-3 Yrs  \n",
       "10             NatWest Group             2-7 Yrs  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for our data\n",
    "df = pd.DataFrame({'JOB TITLE':job_title3,'LOB LOCATION':job_location3,'COMPANY NAME':company_name3,'EXPERIENCE REQUIRED':experience_required3},index=list(range(1,11)))\n",
    "print(\"Data for “Data Scientist” Job position in “Delhi / NCR” location and Salary (3-6 lakhs):\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6c9381",
   "metadata": {},
   "source": [
    "### Q4: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103d8c9c",
   "metadata": {},
   "source": [
    "Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price                                                                                                                       \n",
    "   The attributes which you have to scrape is ticked marked in the below image.\n",
    "* To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and\n",
    "   click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "   required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "   click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a1e0b2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.4.3)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "# installing selenium\n",
    "!pip install selenium\n",
    "\n",
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73236659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection to webdriver\n",
    "driver = webdriver.Chrome(r'chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "70d69409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the flipkart page on automated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "254396ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# canceling the login popup\n",
    "login = driver.find_element(By.XPATH,\"/html/body/div[2]/div/div/button\")\n",
    "login.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d9c821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter “sunglasses” in the search field\n",
    "search_field = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "search_field.send_keys(\"sunglasses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0b4631df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on search button\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc96196",
   "metadata": {},
   "source": [
    "* Reached to sunglasses page \n",
    "* moving to scrap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6b439a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists to store data\n",
    "brand = []\n",
    "product_description = []\n",
    "price = []\n",
    "less_price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b32b6fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping brands of first 100 sunglasses\n",
    "start = 0\n",
    "end = 3\n",
    "for page in range(start,end):\n",
    "    glasses_brand = driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for i in glasses_brand:\n",
    "        glass = i.text\n",
    "        brand.append(glass)\n",
    "    next_button = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3) \n",
    "brand = brand[0:100]    \n",
    "\n",
    "# scraping product description of first 100 sunglasses\n",
    "for page in range(start,end):\n",
    "    discription = driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    for i in discription:\n",
    "        product_description.append(i.text)\n",
    "    next_button = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "product_description = product_description[0:100]    \n",
    " \n",
    "# scraping price of first 100 sunglasses\n",
    "for page in range(start,end):\n",
    "    glass_price = driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    for i in glass_price:\n",
    "        price.append(i.text)\n",
    "    next_button = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "price = price[0:100]\n",
    "\n",
    "# scraping discount on first 100 sunglasses\n",
    "for page in range(start,end):\n",
    "    less_glass_price = driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "    for i in less_glass_price:\n",
    "        less_price.append(i.text)\n",
    "    next_button = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "less_price = less_price[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1c65dd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "# checking length of our lists\n",
    "print(len(brand),len(product_description),len(price),len(less_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "580f1896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of first 100 'SunGlasses' on Flipkart :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>PRODUCT DISCRIPTION</th>\n",
       "      <th>PRICE(After Discount)</th>\n",
       "      <th>DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>UV Protection, Gradient Round Sunglasses (Free...</td>\n",
       "      <td>₹949</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Round Sun...</td>\n",
       "      <td>₹949</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹710</td>\n",
       "      <td>90% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹499</td>\n",
       "      <td>20% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹189</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>Riding Glasses, Night Vision Spectacle Sunglas...</td>\n",
       "      <td>₹220</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Aviator Sunglasses (Free Size)</td>\n",
       "      <td>₹399</td>\n",
       "      <td>20% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Retro Squ...</td>\n",
       "      <td>₹1,533</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹719</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹269</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              BRAND                                PRODUCT DISCRIPTION  \\\n",
       "1     VINCENT CHASE  UV Protection, Gradient Round Sunglasses (Free...   \n",
       "2     VINCENT CHASE  by Lenskart Polarized, UV Protection Round Sun...   \n",
       "3          Fastrack   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "4    ROZZETTA CRAFT      UV Protection Wayfarer Sunglasses (Free Size)   \n",
       "5    kingsunglasses                UV Protection Round Sunglasses (54)   \n",
       "..              ...                                                ...   \n",
       "96             SRPM  Riding Glasses, Night Vision Spectacle Sunglas...   \n",
       "97   ROZZETTA CRAFT       UV Protection Aviator Sunglasses (Free Size)   \n",
       "98    VINCENT CHASE  by Lenskart Polarized, UV Protection Retro Squ...   \n",
       "99         Fastrack              UV Protection Aviator Sunglasses (54)   \n",
       "100  kingsunglasses              UV Protection Aviator Sunglasses (54)   \n",
       "\n",
       "    PRICE(After Discount) DISCOUNT  \n",
       "1                    ₹949  70% off  \n",
       "2                    ₹949  80% off  \n",
       "3                    ₹710  90% off  \n",
       "4                    ₹499  20% off  \n",
       "5                    ₹189  88% off  \n",
       "..                    ...      ...  \n",
       "96                   ₹220  80% off  \n",
       "97                   ₹399  20% off  \n",
       "98                 ₹1,533  60% off  \n",
       "99                   ₹719  84% off  \n",
       "100                  ₹269  82% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for our data\n",
    "df = pd.DataFrame({'BRAND':brand,'PRODUCT DISCRIPTION':product_description,'PRICE(After Discount)':price,'DISCOUNT':less_price},index=list(range(1,101)))\n",
    "print(\"Details of first 100 'SunGlasses' on Flipkart :\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d82094",
   "metadata": {},
   "source": [
    "# Q5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb31f4",
   "metadata": {},
   "source": [
    "Scrape 100 reviews data from flipkart.com for iphone11 phone.                                                               \n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.flipkart.com/\n",
    "2. Enter “iphone 11” in “Search” field .\n",
    "3. Then click the search button.\n",
    "* As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61e45d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.4.3)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "# installing selenium\n",
    "!pip install selenium\n",
    "\n",
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de605d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection to webdriver\n",
    "driver = webdriver.Chrome(r'chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11fb531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the flipkart page on automated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c376e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# canceling the login popup\n",
    "login = driver.find_element(By.XPATH,\"/html/body/div[2]/div/div/button\")\n",
    "login.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0044b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter “iphone 11” in the search field\n",
    "search_field = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "search_field.send_keys(\"iphone 11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899934d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on search button\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45864025",
   "metadata": {},
   "source": [
    "* Reached to iphone 11 page \n",
    "* moving to scrap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58199bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on reviews\n",
    "rev = driver.find_element(By.XPATH,'//span[@class=\"_2_R_DZ\"]')\n",
    "rev.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fd1a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifting to next windows\n",
    "driver.switch_to.window(driver.window_handles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea485601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# click on all reviews\n",
    "all_rev = driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[9]/div[6]/div/a/div/span')\n",
    "all_rev.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e909cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crating empty lists to store data\n",
    "rating_of_phone = []\n",
    "review_summary = []\n",
    "full_review = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bc49ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start scraping\n",
    "start = 0 \n",
    "end  = 10\n",
    "\n",
    "# scraping rating of iphone\n",
    "for page in range(start,end):\n",
    "    P_rating = driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for i in P_rating:\n",
    "        rating_of_phone.append(i.text)\n",
    "    next_button = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c8afccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping review summary\n",
    "for page in range(start,end):\n",
    "    rs = driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "    for i in rs:\n",
    "        review_summary.append(i.text)\n",
    "    next_button = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23d0c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving to first page\n",
    "f_p = driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[2]')\n",
    "f_p.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50d91be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping full summary of iphone 11\n",
    "for page in range(start,end):\n",
    "    frs = driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "    for i in frs:\n",
    "        full_review.append(i.text)\n",
    "    next_button = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9757118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 100 100\n"
     ]
    }
   ],
   "source": [
    "# checking length of our lists\n",
    "print(len(rating_of_phone),len(review_summary),len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f80c074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews on iphone 11 from first 100 people on flipkart :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RATING OF IPHONE 11</th>\n",
       "      <th>REVIEW SUMMARY</th>\n",
       "      <th>FULL REVIEW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>What a camera .....just awesome ..you can feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>Iphone is just awesome.. battery backup is ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing camera quality as expected, battery al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Really worth of money. i just love it. It is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>This is my first ever I phone. Before this I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>5</td>\n",
       "      <td>Good choice</td>\n",
       "      <td>Best budget Iphone till date ❤️ go for it guys...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    RATING OF IPHONE 11      REVIEW SUMMARY  \\\n",
       "1                     4  Highly recommended   \n",
       "2                     5    Perfect product!   \n",
       "3                     5    Perfect product!   \n",
       "4                     5    Perfect product!   \n",
       "5                     5   Worth every penny   \n",
       "..                  ...                 ...   \n",
       "96                    5           Fabulous!   \n",
       "97                    5       Great product   \n",
       "98                    5   Worth every penny   \n",
       "99                    5  Highly recommended   \n",
       "100                   5         Good choice   \n",
       "\n",
       "                                           FULL REVIEW  \n",
       "1    Really satisfied with the Product I received.....  \n",
       "2    Amazing phone with great cameras and better ba...  \n",
       "3    Great iPhone very snappy experience as apple k...  \n",
       "4    Previously I was using one plus 3t it was a gr...  \n",
       "5    What a camera .....just awesome ..you can feel...  \n",
       "..                                                 ...  \n",
       "96   Iphone is just awesome.. battery backup is ver...  \n",
       "97   Amazing camera quality as expected, battery al...  \n",
       "98   Really worth of money. i just love it. It is t...  \n",
       "99   This is my first ever I phone. Before this I w...  \n",
       "100  Best budget Iphone till date ❤️ go for it guys...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for our data\n",
    "df = pd.DataFrame({'RATING OF IPHONE 11':rating_of_phone[0:100],'REVIEW SUMMARY':review_summary[0:100],'FULL REVIEW':full_review[0:100]},index=list(range(1,101)))\n",
    "print(\"Reviews on iphone 11 from first 100 people on flipkart :\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18771a",
   "metadata": {},
   "source": [
    "### Q6: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c88c2ab",
   "metadata": {},
   "source": [
    "Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "59a674cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.4.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "# installing selenium\n",
    "!pip install selenium\n",
    "\n",
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8026daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection to webdriver\n",
    "driver = webdriver.Chrome(r'chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b379cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the flipkart page on automated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1e72289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# canceling the login popup\n",
    "login = driver.find_element(By.XPATH,\"/html/body/div[2]/div/div/button\")\n",
    "login.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b782cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter “iphone 11” in the search field\n",
    "search_field = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "search_field.send_keys(\"sneakers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "69dcc511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on search button\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23adeda",
   "metadata": {},
   "source": [
    "* Reached to sneakers page\n",
    "* moving to scrap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5932ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists to store data\n",
    "sbrand = []\n",
    "sproduct_description = []\n",
    "sprice = []\n",
    "sless_price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8993833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping brands of first 100 sneakers\n",
    "start = 0\n",
    "end = 3\n",
    "for page in range(start,end):\n",
    "    sneaker_brand = driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for i in sneaker_brand:\n",
    "        glass = i.text\n",
    "        sbrand.append(glass)\n",
    "    next_button = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3) \n",
    "sbrand = sbrand[0:100]    \n",
    "\n",
    "# scraping product description of first 100 sneakers\n",
    "for page in range(start,end):\n",
    "    sdiscription = driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    for i in sdiscription:\n",
    "        sproduct_description.append(i.text)\n",
    "    next_button = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "sproduct_description = sproduct_description[0:100]    \n",
    " \n",
    "# scraping price of first 100 sneakers\n",
    "for page in range(start,end):\n",
    "    sglass_price = driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    for i in sglass_price:\n",
    "        sprice.append(i.text)\n",
    "    next_button = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "sprice = sprice[0:100]\n",
    "\n",
    "# scraping discount on first 100 sunglasses\n",
    "for page in range(start,end):\n",
    "    sless_glass_price = driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "    for i in sless_glass_price:\n",
    "        sless_price.append(i.text)\n",
    "    next_button = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "sless_price = sless_price[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "85110cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "# checking length of our lists\n",
    "print(len(sbrand),len(sproduct_description),len(sprice),len(sless_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8e0c25ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of first 100 'Sneakers' on Flipkart :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>PRODUCT DISCRIPTION</th>\n",
       "      <th>PRICE(After Discount)</th>\n",
       "      <th>DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Magnolia</td>\n",
       "      <td>Men's Sneakers Fashion Lightweight Running Sho...</td>\n",
       "      <td>₹419</td>\n",
       "      <td>40% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Labbin</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹449</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KWIK FIT</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹357</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DUCATI</td>\n",
       "      <td>Original Luxury Branded Fashionable Men's Casu...</td>\n",
       "      <td>₹971</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aadi</td>\n",
       "      <td>Rebound JOY Sneakers For Men</td>\n",
       "      <td>₹288</td>\n",
       "      <td>45% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ASTEROID</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹474</td>\n",
       "      <td>34% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>CR-1 Sneakers For Men</td>\n",
       "      <td>₹499</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Deals4you</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹383</td>\n",
       "      <td>57% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Combo Pack Of 2 Latest Stylish Casual Shoes fo...</td>\n",
       "      <td>₹199</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Comfortable &amp; Ultra Light Weight Sneaker Sneak...</td>\n",
       "      <td>₹376</td>\n",
       "      <td>66% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         BRAND                                PRODUCT DISCRIPTION  \\\n",
       "1     Magnolia  Men's Sneakers Fashion Lightweight Running Sho...   \n",
       "2       Labbin                                   Sneakers For Men   \n",
       "3     KWIK FIT                                   Sneakers For Men   \n",
       "4       DUCATI  Original Luxury Branded Fashionable Men's Casu...   \n",
       "5         aadi                       Rebound JOY Sneakers For Men   \n",
       "..         ...                                                ...   \n",
       "96    ASTEROID                                   Sneakers For Men   \n",
       "97      Chevit                              CR-1 Sneakers For Men   \n",
       "98   Deals4you                                   Sneakers For Men   \n",
       "99      BRUTON  Combo Pack Of 2 Latest Stylish Casual Shoes fo...   \n",
       "100     BRUTON  Comfortable & Ultra Light Weight Sneaker Sneak...   \n",
       "\n",
       "    PRICE(After Discount) DISCOUNT  \n",
       "1                    ₹419  40% off  \n",
       "2                    ₹449  60% off  \n",
       "3                    ₹357  62% off  \n",
       "4                    ₹971  52% off  \n",
       "5                    ₹288  45% off  \n",
       "..                    ...      ...  \n",
       "96                   ₹474  34% off  \n",
       "97                   ₹499  78% off  \n",
       "98                   ₹383  57% off  \n",
       "99                   ₹199  52% off  \n",
       "100                  ₹376  66% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'BRAND':sbrand,'PRODUCT DISCRIPTION':sproduct_description,'PRICE(After Discount)':sprice,'DISCOUNT':sless_price},index=list(range(1,101)))\n",
    "print(\"Details of first 100 'Sneakers' on Flipkart :\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfcdd9f",
   "metadata": {},
   "source": [
    "### Q7:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb06afd6",
   "metadata": {},
   "source": [
    "Go to the link - https://www.myntra.com/shoes\n",
    "Set second Price filter and Color filter to “Black”, as shown in the below image.                                               \n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe                         \n",
    "description, price of the shoe as shown in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f1eea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.4.3)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "# installing selenium\n",
    "!pip install selenium\n",
    "\n",
    "# installing important libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "be3e547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the driver \n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b1a83bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the site\n",
    "driver.get(\"https://www.myntra.com/shoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4f0f1f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting color filter to black\n",
    "color_filter = driver.find_element(By.XPATH,'/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]')\n",
    "color_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "64800bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting second price filter\n",
    "price_filter = driver.find_element(By.XPATH,'/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label/div')\n",
    "price_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a1150526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lists to store data\n",
    "shoe_brand = []\n",
    "shoe_discrip = []\n",
    "shoe_price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "87c8f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping the data\n",
    "start = 0\n",
    "end = 2\n",
    "# scraping shoe brand\n",
    "for page in range(start,end):\n",
    "    shoe_b = driver.find_elements(By.XPATH,'//h3[@class=\"product-brand\"]')\n",
    "    for i in shoe_b:\n",
    "        shoe_brand.append(i.text)\n",
    "    next_button = driver.find_element(By.XPATH,'//span[@class=\"pagination-arrowRight\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3) \n",
    "shoe_brand = shoe_brand[0:100]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "97ced5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to come back at first page\n",
    "first = driver.find_element(By.XPATH,'/html/body/div[2]/div/div[1]/main/div[3]/div[2]/div/div[2]/section/div[2]/ul/li[3]')\n",
    "first.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ff3a89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping product description of first 100 shoes\n",
    "for page in range(start,end):\n",
    "    shoe_discription = driver.find_elements(By.XPATH,'//h4[@class=\"product-product\"]')\n",
    "    for i in shoe_discription:\n",
    "        shoe_discrip.append(i.text)\n",
    "    next_button = driver.find_element(By.XPATH,'//span[@class=\"pagination-arrowRight\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "shoe_discrip = shoe_discrip[0:100]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "5246ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to come back at first page\n",
    "first = driver.find_element(By.XPATH,'/html/body/div[2]/div/div[1]/main/div[3]/div[2]/div/div[2]/section/div[2]/ul/li[3]')\n",
    "first.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "81168aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping price of first 100 shoes\n",
    "for page in range(start,end):\n",
    "    s_price = driver.find_elements(By.XPATH,'//div[@class=\"product-price\"]')\n",
    "    for i in s_price:\n",
    "        shoe_price.append(i.text)\n",
    "    next_button = driver.find_element(By.XPATH,'//span[@class=\"pagination-arrowRight\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "shoe_price = shoe_price[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "c29c0789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "# checking length of our lists\n",
    "print(len(shoe_brand),len(shoe_discrip),len(shoe_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "3b771638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST 100 'BLACK' SHOES DESCRIPTION UNDER (Rs. 7187 to Rs. 14125) on MYNTRA :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>SHOES DESCRIPTION</th>\n",
       "      <th>SHOES PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men ZOOM WINFLO8 Running Shoes</td>\n",
       "      <td>Rs. 7880Rs. 8295(5% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men React Infinity 3 Running</td>\n",
       "      <td>Rs. 13295Rs. 13995(5% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men JORDAN LOW Basketball Shoe</td>\n",
       "      <td>Rs. 12795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Solid Leather Formal Slip-Ons</td>\n",
       "      <td>Rs. 7649Rs. 8999(15% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>Men Ultraboost 5.0 DNA Running</td>\n",
       "      <td>Rs. 12749Rs. 16999(25% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Royal Enfield</td>\n",
       "      <td>Cabo WP Riding Boots</td>\n",
       "      <td>Rs. 8500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ECCO</td>\n",
       "      <td>Leather Block Heels</td>\n",
       "      <td>Rs. 13999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>J.FONTINI</td>\n",
       "      <td>Men Leather Formal Loafers</td>\n",
       "      <td>Rs. 7490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Heel &amp; Buckle London</td>\n",
       "      <td>Men Leather Sneakers</td>\n",
       "      <td>Rs. 11192Rs. 13990(20% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>DAVINCHI</td>\n",
       "      <td>Men Formal Leather Slip-Ons</td>\n",
       "      <td>Rs. 8990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    BRAND                  SHOES DESCRIPTION  \\\n",
       "1                    Nike     Men ZOOM WINFLO8 Running Shoes   \n",
       "2                    Nike       Men React Infinity 3 Running   \n",
       "3                    Nike     Men JORDAN LOW Basketball Shoe   \n",
       "4            Hush Puppies  Men Solid Leather Formal Slip-Ons   \n",
       "5                  ADIDAS     Men Ultraboost 5.0 DNA Running   \n",
       "..                    ...                                ...   \n",
       "96          Royal Enfield               Cabo WP Riding Boots   \n",
       "97                   ECCO                Leather Block Heels   \n",
       "98              J.FONTINI         Men Leather Formal Loafers   \n",
       "99   Heel & Buckle London               Men Leather Sneakers   \n",
       "100              DAVINCHI        Men Formal Leather Slip-Ons   \n",
       "\n",
       "                     SHOES PRICE  \n",
       "1       Rs. 7880Rs. 8295(5% OFF)  \n",
       "2     Rs. 13295Rs. 13995(5% OFF)  \n",
       "3                      Rs. 12795  \n",
       "4      Rs. 7649Rs. 8999(15% OFF)  \n",
       "5    Rs. 12749Rs. 16999(25% OFF)  \n",
       "..                           ...  \n",
       "96                      Rs. 8500  \n",
       "97                     Rs. 13999  \n",
       "98                      Rs. 7490  \n",
       "99   Rs. 11192Rs. 13990(20% OFF)  \n",
       "100                     Rs. 8990  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for our data\n",
    "df = pd.DataFrame({'BRAND':shoe_brand,'SHOES DESCRIPTION':shoe_discrip,'SHOES PRICE':shoe_price},index=list(range(1,101)))\n",
    "print(\"FIRST 100 'BLACK' SHOES DESCRIPTION UNDER (Rs. 7187 to Rs. 14125) on MYNTRA :\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c9df1",
   "metadata": {},
   "source": [
    "### Q8: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e0f38",
   "metadata": {},
   "source": [
    "Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c40d6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.4.3)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "# installing selenium\n",
    "!pip install selenium\n",
    "\n",
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7432ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection to webdriver\n",
    "driver = webdriver.Chrome(r'chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80a7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the site on automated chrome browser\n",
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28feac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter “laptop” in the search field\n",
    "search_field = driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "search_field.send_keys(\"Laptop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbd13bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on search\n",
    "search = driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94d0da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting CPU Type filter to “Intel Core i7”\n",
    "filter_cpu = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[5]/ul[4]/li[12]/span/a/span')\n",
    "filter_cpu.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23f6d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list to store the data\n",
    "lap_title = []\n",
    "lap_rating = []\n",
    "lap_price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec2e8f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now i am on the location and starting the scraping of asked data\n",
    "\n",
    "# scraping title\n",
    "title_l = driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "for i in title_l[0:10]:\n",
    "    k = i.text\n",
    "    lap_title.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23a9dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping price of laptop\n",
    "l_price = driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "for i in l_price[0:10]:\n",
    "    company = i.text\n",
    "    lap_price.append(company)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0746f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping laptop rating       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "494bb9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to scrape rating of 1st laptop\n",
    "click1 = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[3]/div/div/div/div/div/div[2]/div/div/div[2]/div/span[1]/span/a/i[1]')\n",
    "click1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c0ef9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate1 = driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-beside-button a-text-bold\"]')\n",
    "for i in rate1:\n",
    "    lap_rating.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b38cb387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free click\n",
    "free_c = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[3]/div/div/div/div/div/div[2]/div/div/div[3]/div[2]/div')\n",
    "free_c.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a3395bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to scrape 2nd laptop rating\n",
    "click2 = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[4]/div/div/div/div/div/div[2]/div/div/div[2]/div/span[1]/span/a/i[1]')\n",
    "click2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab9efa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate2 = driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-beside-button a-text-bold\"]')\n",
    "for i in rate2:\n",
    "    lap_rating.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "792d4565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free click\n",
    "free_c = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[3]/div/div/div/div/div/div[2]/div/div/div[3]/div[2]/div')\n",
    "free_c.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b586ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to scrape 3rd laptop rating\n",
    "click3 = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[5]/div/div/div/div/div/div[2]/div/div/div[2]/div/span[1]/span/a/i[1]')\n",
    "click3.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e327fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate3 = driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-beside-button a-text-bold\"]')\n",
    "for i in rate3:\n",
    "    lap_rating.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "305ea7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free click\n",
    "free_c = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[3]/div/div/div/div/div/div[2]/div/div/div[3]/div[2]/div')\n",
    "free_c.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "057a2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to scrape 4th laptop rating\n",
    "click4 = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[7]/div/div/div/div/div/div[2]/div/div/div[2]/div/span[1]/span/a/i[1]')\n",
    "click4.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "134dab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate4 = driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-beside-button a-text-bold\"]')\n",
    "for i in rate4:\n",
    "    lap_rating.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0844dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free click\n",
    "free_c = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[3]/div/div/div/div/div/div[2]/div/div/div[3]/div[2]/div')\n",
    "free_c.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd874a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to scrape 5th laptop rating\n",
    "click5 = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[8]/div/div/div/div/div/div[2]/div/div/div[2]/div/span[1]/span/a/i[1]')\n",
    "click5.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b8ada6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate5 = driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-beside-button a-text-bold\"]')\n",
    "for i in rate5:\n",
    "    lap_rating.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e2f2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free click\n",
    "free_c = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[3]/div/div/div/div/div/div[2]/div/div/div[3]/div[2]/div')\n",
    "free_c.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9cac163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to scrape 6th laptop rating\n",
    "click6 = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[9]/div/div/div/div/div/div[2]/div/div/div[2]/div/span[1]/span/a/i[1]')\n",
    "click6.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "385ee6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate6 = driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-beside-button a-text-bold\"]')\n",
    "for i in rate6:\n",
    "    lap_rating.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18fd6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free click\n",
    "free_c = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[3]/div/div/div/div/div/div[2]/div/div/div[3]/div[2]/div')\n",
    "free_c.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33d63fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7th laptop has no rating so i will write no rating there\n",
    "lap_rating.append('no rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd4f9fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to scrape 8th laptop rating\n",
    "click8 = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[12]/div/div/div/div/div/div[2]/div/div/div[2]/div/span[1]/span/a/i[1]')\n",
    "click8.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d625ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate8 = driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-beside-button a-text-bold\"]')\n",
    "for i in rate8:\n",
    "    lap_rating.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8cd413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free click\n",
    "free_c = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[3]/div/div/div/div/div/div[2]/div/div/div[3]/div[2]/div')\n",
    "free_c.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd990796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to scrape 9th laptop rating\n",
    "click9 = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[12]/div/div/div/div/div/div[2]/div/div/div[2]/div/span[1]/span/a/i[1]')\n",
    "click9.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "852879e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate9 = driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-beside-button a-text-bold\"]')\n",
    "for i in rate9:\n",
    "    lap_rating.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64c26774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free click\n",
    "free_c = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[3]/div/div/div/div/div/div[2]/div/div/div[3]/div[2]/div')\n",
    "free_c.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "122e829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to scrape 10th laptop rating\n",
    "click10 = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[13]/div/div/div/div/div/div[2]/div/div/div[2]/div/span[1]/span/a/i[1]')\n",
    "click10.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6a431d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate10 = driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-beside-button a-text-bold\"]')\n",
    "for i in rate10:\n",
    "    lap_rating.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8d1e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free click\n",
    "free_c = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[3]/div/div/div/div/div/div[2]/div/div/div[3]/div[2]/div')\n",
    "free_c.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cae0d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=lap_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fe3d8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(l)):\n",
    "    if i=='':\n",
    "        l.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "edce49fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3.9 out of 5',\n",
       " '4.4 out of 5',\n",
       " '3.3 out of 5',\n",
       " '3.8 out of 5',\n",
       " '4.8 out of 5',\n",
       " '4.3 out of 5',\n",
       " 'no rating',\n",
       " '4 out of 5',\n",
       " '4 out of 5',\n",
       " '4.7 out of 5']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "153c603c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "# checking lenght of the lists\n",
    "print(len(lap_title),len(l),len(lap_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4883dc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETAILS OF FIRST 10 i7 LAPTOPS ON AMAZON \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LAPTOP</th>\n",
       "      <th>LAPTOP RATING</th>\n",
       "      <th>LAPTOP PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lenovo Ideapad Gaming 3 Intel Core i7 10th Gen...</td>\n",
       "      <td>3.9 out of 5</td>\n",
       "      <td>62,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lenovo Yoga 7i 11th Gen Intel Core i7-1165G7 1...</td>\n",
       "      <td>4.4 out of 5</td>\n",
       "      <td>1,06,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Renewed) Lenovo Intel Core i7 5600U 12.5-Inch...</td>\n",
       "      <td>3.3 out of 5</td>\n",
       "      <td>26,998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASUS Vivobook 15, 15.6-inch (39.62 cms) FHD, I...</td>\n",
       "      <td>3.8 out of 5</td>\n",
       "      <td>57,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lenovo IdeaPad Slim 5 Intel Core i7 12th Gen 1...</td>\n",
       "      <td>4.8 out of 5</td>\n",
       "      <td>81,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....</td>\n",
       "      <td>4.3 out of 5</td>\n",
       "      <td>80,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Renewed) HP Elitebook 840G1-i7-8 GB-2 TB 14-i...</td>\n",
       "      <td>no rating</td>\n",
       "      <td>41,690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HP Victus 11th Gen Intel Core i7-11800H 16.1\" ...</td>\n",
       "      <td>4 out of 5</td>\n",
       "      <td>1,01,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ASUS TUF Gaming F15 (2022), 15.6\" (39.62 cms) ...</td>\n",
       "      <td>4 out of 5</td>\n",
       "      <td>1,07,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ASUS TUF Dash F15 (2022), 15.6-inch (39.62 cms...</td>\n",
       "      <td>4.7 out of 5</td>\n",
       "      <td>1,26,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               LAPTOP LAPTOP RATING  \\\n",
       "1   Lenovo Ideapad Gaming 3 Intel Core i7 10th Gen...  3.9 out of 5   \n",
       "2   Lenovo Yoga 7i 11th Gen Intel Core i7-1165G7 1...  4.4 out of 5   \n",
       "3   (Renewed) Lenovo Intel Core i7 5600U 12.5-Inch...  3.3 out of 5   \n",
       "4   ASUS Vivobook 15, 15.6-inch (39.62 cms) FHD, I...  3.8 out of 5   \n",
       "5   Lenovo IdeaPad Slim 5 Intel Core i7 12th Gen 1...  4.8 out of 5   \n",
       "6   Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....  4.3 out of 5   \n",
       "7   (Renewed) HP Elitebook 840G1-i7-8 GB-2 TB 14-i...     no rating   \n",
       "8   HP Victus 11th Gen Intel Core i7-11800H 16.1\" ...    4 out of 5   \n",
       "9   ASUS TUF Gaming F15 (2022), 15.6\" (39.62 cms) ...    4 out of 5   \n",
       "10  ASUS TUF Dash F15 (2022), 15.6-inch (39.62 cms...  4.7 out of 5   \n",
       "\n",
       "   LAPTOP PRICE  \n",
       "1        62,990  \n",
       "2      1,06,999  \n",
       "3        26,998  \n",
       "4        57,990  \n",
       "5        81,990  \n",
       "6        80,990  \n",
       "7        41,690  \n",
       "8      1,01,490  \n",
       "9      1,07,990  \n",
       "10     1,26,990  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for our list\n",
    "df = pd.DataFrame({'LAPTOP':lap_title,'LAPTOP RATING':l,'LAPTOP PRICE':lap_price},index=list(range(1,11)))\n",
    "print(\"DETAILS OF FIRST 10 i7 LAPTOPS ON AMAZON \")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae1f73",
   "metadata": {},
   "source": [
    "### Q9: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cc0819",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter\n",
    "   “Data Scientist” and click on search button.\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter\n",
    "   “Noida” and select location “Noida”.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "05783349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.4.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "# installing selenium\n",
    "!pip install selenium\n",
    "\n",
    "# installing important libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "46f06b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the driver \n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "90e731e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the site\n",
    "driver.get(\"https://www.ambitionbox.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "0cb423f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on the close button of the login page to reach the main pain\n",
    "cancel = driver.find_element(By.XPATH,'/html/body/section[10]/div[1]/div[2]/a/i')\n",
    "cancel.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "51ad06ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on job button\n",
    "job_button = driver.find_element(By.XPATH,'/html/body/div[1]/nav[2]/div/ul/li[5]/a')\n",
    "job_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "2b98cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter 'data scientist' on search bar\n",
    "search_bar = driver.find_element(By.XPATH,'/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/div/span/input')\n",
    "search_bar.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "edbe075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on search button\n",
    "search_button = driver.find_element(By.XPATH,'/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/button/span')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "5696ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on location\n",
    "location_selector = driver.find_element(By.XPATH,'/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[1]/p')\n",
    "location_selector.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "a2a25723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter 'noida' in search location\n",
    "enter_location = driver.find_element(By.XPATH,'/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[2]/input')\n",
    "enter_location.send_keys('Noida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "ce3f8aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 'noida' from the options\n",
    "select_nodia = driver.find_element(By.XPATH,'/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[3]/div[1]/div[1]/div/label')\n",
    "select_nodia.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "3b533c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists to store data\n",
    "company_name = []\n",
    "no_of_days = []\n",
    "rating_of_company = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "dc3a88cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping company name, No. of days ago when job was posted, Rating of the company.\n",
    "company_n = driver.find_elements(By.XPATH,'//p[@class=\"company body-medium\"]')\n",
    "for i in company_n[0:10]:\n",
    "    company_name.append(i.text)\n",
    "\n",
    "# scraping no of days when job was posted\n",
    "days = driver.find_elements(By.XPATH,'//span[@class=\"body-small-l\"]')\n",
    "for i in days[0:20]:\n",
    "    no_of_days.append(i.text)\n",
    "no_of_days = no_of_days[0:20:2]        \n",
    "\n",
    "# scraping rating of the company\n",
    "rating = driver.find_elements(By.XPATH,'//span[@class=\"body-small\"]')\n",
    "for i in rating[0:10]:\n",
    "    rating_of_company.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "8b220a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "# checking length of our lists\n",
    "print(len(company_name),len(no_of_days),len(rating_of_company))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "082ff9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data for first 10 job results for Data Scientist Designation in Noida location. (AMBITIONBOX) :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPANY NAME</th>\n",
       "      <th>JOB POSTED ON AMBITIONBOX</th>\n",
       "      <th>RATING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBRE South Asia Pvt Ltd</td>\n",
       "      <td>25d ago</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>20d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Genpact</td>\n",
       "      <td>28d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ericsson India Global Services Pvt. Ltd.</td>\n",
       "      <td>1mon ago</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>1mon ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dew Solutions Pvt. Ltd.</td>\n",
       "      <td>25d ago</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ericsson India Global Services Pvt. Ltd.</td>\n",
       "      <td>18d ago</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>One97 Communications Limited</td>\n",
       "      <td>20d ago</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EY</td>\n",
       "      <td>1mon ago</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>R Systems International Ltd.</td>\n",
       "      <td>28d ago</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                COMPANY NAME JOB POSTED ON AMBITIONBOX RATING\n",
       "1                    CBRE South Asia Pvt Ltd                   25d ago    4.3\n",
       "2              GENPACT India Private Limited                   20d ago    4.0\n",
       "3                                    Genpact                   28d ago    4.0\n",
       "4   Ericsson India Global Services Pvt. Ltd.                  1mon ago    4.3\n",
       "5              GENPACT India Private Limited                  1mon ago    4.0\n",
       "6                    Dew Solutions Pvt. Ltd.                   25d ago    4.3\n",
       "7   Ericsson India Global Services Pvt. Ltd.                   18d ago    4.3\n",
       "8               One97 Communications Limited                   20d ago    3.8\n",
       "9                                         EY                  1mon ago    3.8\n",
       "10              R Systems International Ltd.                   28d ago    3.7"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create data for our data\n",
    "df = pd.DataFrame({'COMPANY NAME':company_name,'JOB POSTED ON AMBITIONBOX':no_of_days,'RATING':rating_of_company},index=list(range(1,11)))\n",
    "print(\"data for first 10 job results for Data Scientist Designation in Noida location. (AMBITIONBOX) :\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934fd1b3",
   "metadata": {},
   "source": [
    "### Q10: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cf7da4",
   "metadata": {},
   "source": [
    "Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the salaries option as shown in the image.\n",
    "3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and\n",
    "   then click on “Data Scientist”.\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average\n",
    "   salary, minimum salary, maximum salary, experience required.\n",
    "5. Store the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "1418a27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.4.3)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "# installing selenium\n",
    "!pip install selenium\n",
    "\n",
    "# installing important libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f097f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the driver \n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "84bddaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the site\n",
    "driver.get(\"https://www.ambitionbox.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bb5703f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on the close button of the login page to reach the main pain\n",
    "cancel = driver.find_element(By.XPATH,'/html/body/section[10]/div[1]/div[2]/a/i')\n",
    "cancel.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "18b68f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on the salary option\n",
    "salary_button = driver.find_element(By.XPATH,'/html/body/div[1]/nav[2]/div/ul/li[3]/a')\n",
    "salary_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bc760fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on browse salary\n",
    "browse = driver.find_element(By.XPATH,'/html/body/div[1]/nav[2]/div/ul/li[3]/div/ul/li[1]/div/div[2]/p')\n",
    "browse.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6bf6ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter desgination data scientist\n",
    "desig  = driver.find_element(By.XPATH,'/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/input')\n",
    "desig.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cb16aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on search buttom\n",
    "dsearch = driver.find_element(By.XPATH,'/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/div/div/div[1]/div/div/p')\n",
    "dsearch.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "01c3c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list to store data\n",
    "companys_name = []\n",
    "total = []\n",
    "avg = []\n",
    "exp = []\n",
    "min_max = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c16b0977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, scraping the data\n",
    "\n",
    "# scraping company name\n",
    "c_name = driver.find_elements(By.XPATH,'//a[@data-v-4c07f399]')\n",
    "for i in c_name[0:10]:\n",
    "    companys_name.append(i.text.replace('\\nData Scientist Salary',\" \"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "06f55620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping total salary records\n",
    "total_sal = driver.find_elements(By.XPATH,'//span[@class=\"datapoints\"]')\n",
    "for i in total_sal[0:10]:\n",
    "    total.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5379b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping min salary\n",
    "# min_sal = driver.find_elements(By.XPATH,'//div[@class=\"value body-medium\"]')\n",
    "# for i in min_sal[0:10]:\n",
    "#    minimun.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2db7d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of scraping min salary and max salary separately i am scraping it together it will look good in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5267015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping min to max salary\n",
    "max_sal = driver.find_elements(By.XPATH,'//div[@class=\"salary-values\"]')\n",
    "for i in max_sal[0:10]:\n",
    "    min_max.append(i.text.replace('\\n',\" to \"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ae869f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping avg salary\n",
    "avg_sal = driver.find_elements(By.XPATH,'//p[@class=\"averageCtc\"]')\n",
    "for i in avg_sal[0:10]:\n",
    "    avg.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e1f20394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping experience\n",
    "expe = driver.find_elements(By.XPATH,'//div[@class=\"sbold-list-header\"]')\n",
    "for i in expe[0:10]:\n",
    "    exp.append(i.text.replace(' (based on ',' ').replace(' salaries)',' ').replace('22',' ').replace('54',' ').replace('39',' ').replace('34',' ').replace('116',' ').replace('67',' ').replace('10',' ').replace('11',' ').replace('13',' ').replace('49',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f831d0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "# let's count element in our list\n",
    "print(len(companys_name),len(total),len(avg),len(min_max),len(exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e2e9911f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salaries data for Data Scientist designation :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPANY NAME</th>\n",
       "      <th>SALARY COUNT</th>\n",
       "      <th>AVERAGE SALARY</th>\n",
       "      <th>MINIMUM TO MAXIMUM SALARY</th>\n",
       "      <th>EXPERIENCE REQUIRED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>(based on 22 salaries)</td>\n",
       "      <td>₹ 32.2L</td>\n",
       "      <td>₹ 25.0L to ₹ 45.0L</td>\n",
       "      <td>3-4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ab Inbev</td>\n",
       "      <td>(based on 54 salaries)</td>\n",
       "      <td>₹ 19.8L</td>\n",
       "      <td>₹ 15.0L to ₹ 26.0L</td>\n",
       "      <td>2-4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Optum</td>\n",
       "      <td>(based on 49 salaries)</td>\n",
       "      <td>₹ 16.4L</td>\n",
       "      <td>₹ 11.0L to ₹ 22.6L</td>\n",
       "      <td>2-4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZS</td>\n",
       "      <td>(based on 34 salaries)</td>\n",
       "      <td>₹ 15.8L</td>\n",
       "      <td>₹ 11.0L to ₹ 22.0L</td>\n",
       "      <td>1-2 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>(based on 116 salaries)</td>\n",
       "      <td>₹ 15.5L</td>\n",
       "      <td>₹ 9.0L to ₹ 24.0L</td>\n",
       "      <td>2-4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tiger Analytics</td>\n",
       "      <td>(based on 67 salaries)</td>\n",
       "      <td>₹ 14.7L</td>\n",
       "      <td>₹ 9.0L to ₹ 20.0L</td>\n",
       "      <td>2-4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sigmoid Analytics</td>\n",
       "      <td>(based on 10 salaries)</td>\n",
       "      <td>₹ 14.7L</td>\n",
       "      <td>₹ 12.7L to ₹ 19.7L</td>\n",
       "      <td>1 yr experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Legato Health Technologies</td>\n",
       "      <td>(based on 11 salaries)</td>\n",
       "      <td>₹ 14.5L</td>\n",
       "      <td>₹ 11.0L to ₹ 20.0L</td>\n",
       "      <td>4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tredence</td>\n",
       "      <td>(based on 13 salaries)</td>\n",
       "      <td>₹ 14.1L</td>\n",
       "      <td>₹ 8.8L to ₹ 17.5L</td>\n",
       "      <td>3 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HSBC</td>\n",
       "      <td>(based on 10 salaries)</td>\n",
       "      <td>₹ 14.0L</td>\n",
       "      <td>₹ 12.0L to ₹ 18.0L</td>\n",
       "      <td>4 yrs experience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   COMPANY NAME             SALARY COUNT AVERAGE SALARY  \\\n",
       "1                      Walmart    (based on 22 salaries)        ₹ 32.2L   \n",
       "2                     Ab Inbev    (based on 54 salaries)        ₹ 19.8L   \n",
       "3                        Optum    (based on 49 salaries)        ₹ 16.4L   \n",
       "4                           ZS    (based on 34 salaries)        ₹ 15.8L   \n",
       "5            Fractal Analytics   (based on 116 salaries)        ₹ 15.5L   \n",
       "6              Tiger Analytics    (based on 67 salaries)        ₹ 14.7L   \n",
       "7            Sigmoid Analytics    (based on 10 salaries)        ₹ 14.7L   \n",
       "8   Legato Health Technologies    (based on 11 salaries)        ₹ 14.5L   \n",
       "9                     Tredence    (based on 13 salaries)        ₹ 14.1L   \n",
       "10                        HSBC    (based on 10 salaries)        ₹ 14.0L   \n",
       "\n",
       "   MINIMUM TO MAXIMUM SALARY    EXPERIENCE REQUIRED  \n",
       "1         ₹ 25.0L to ₹ 45.0L  3-4 yrs experience     \n",
       "2         ₹ 15.0L to ₹ 26.0L  2-4 yrs experience     \n",
       "3         ₹ 11.0L to ₹ 22.6L  2-4 yrs experience     \n",
       "4         ₹ 11.0L to ₹ 22.0L  1-2 yrs experience     \n",
       "5          ₹ 9.0L to ₹ 24.0L  2-4 yrs experience     \n",
       "6          ₹ 9.0L to ₹ 20.0L  2-4 yrs experience     \n",
       "7         ₹ 12.7L to ₹ 19.7L     1 yr experience     \n",
       "8         ₹ 11.0L to ₹ 20.0L    4 yrs experience     \n",
       "9          ₹ 8.8L to ₹ 17.5L    3 yrs experience     \n",
       "10        ₹ 12.0L to ₹ 18.0L    4 yrs experience     "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for our data\n",
    "df = pd.DataFrame({'COMPANY NAME':companys_name,'SALARY COUNT':total,'AVERAGE SALARY':avg,'MINIMUM TO MAXIMUM SALARY':min_max,'EXPERIENCE REQUIRED':exp},index=list(range(1,11)))\n",
    "print(\"Salaries data for Data Scientist designation :\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05c8d04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
